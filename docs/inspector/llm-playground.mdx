---
title: "LLM Playground"
description: "Chat with your MCP server against any LLM and test ChatGPT apps and MCP apps"
icon: "Messages"
---

The LLM Playground lets you chat with your MCP server using any LLM. Test how different models interact with your tools, simulate agent behaviors, and develop ChatGPT apps and MCP apps with a full widget emulator. We provide frontier models like GPT-5 and Claude Sonnet for free, or bring your own API key.

<Frame>
  <img
    className="block"
    src="/images/mcpjam-mcp-ui.png"
    alt="MCPJam LLM Playground"
    width="1000"
  />
</Frame>

## Key Features

- **Multi-LLM Support** - Test against OpenAI, Anthropic Claude, Google Gemini, Deepseek, Mistral, Ollama, OpenRouter, and LiteLLM
- **Free Frontier Models** - Access GPT-5, Claude Sonnet, and Gemini 2.5 models for free
- **Widget Emulator** - Full support for ChatGPT apps and MCP apps with inline rendering
- **Real-time Debugging** - Split-panel view with chat on the left and JSON-RPC logs on the right
- **Prompt Library** - Use MCP prompts directly in chat by typing `/`
- **Elicitation Support** - Interactive prompts and forms from your MCP server
- **Customizable Agent** - Configure system prompts and temperature settings

## Setup

To use the playground, configure at least one LLM provider:

1. **Navigate to Settings** - Click the Settings tab in MCPJam Inspector
2. **Choose a Provider** - Select from OpenAI, Anthropic, Gemini, Deepseek, Mistral, Ollama, OpenRouter, or LiteLLM
3. **Add API Key** - Enter your API key for the selected provider
4. **Select Model** - Go to the Playground and choose your model from the dropdown

### Supported Providers

**OpenAI**
Get an API key from [OpenAI Platform](https://platform.openai.com/api-keys). Supports GPT-4, GPT-5, and o-series models.

<Note>
  GPT-5 models require organization verification and do not support temperature configuration.
</Note>

**Anthropic Claude**
Get an API key from [Anthropic Console](https://console.anthropic.com/). Supports Claude Opus, Sonnet, and Haiku models.

**Google Gemini**
Get an API key from [Google AI Studio](https://aistudio.google.com/). Supports Gemini Pro, Flash, and Gemma models.

**Deepseek**
Get an API key from [Deepseek Platform](https://platform.deepseek.com/). Supports deepseek-chat and deepseek-reasoner.

**Mistral AI**
Get an API key from [Mistral AI Console](https://console.mistral.ai/api-keys/). Supports Mistral Large, Codestral, and Ministral models.

**OpenRouter**
Get an API key from [OpenRouter Console](https://openrouter.ai/settings/keys). Access 100+ models through a unified interface.

**Ollama**
Install [Ollama](https://ollama.com/) and start with `ollama serve <model>`. MCPJam automatically detects running models.

**LiteLLM Proxy**
Use [LiteLLM Proxy](https://docs.litellm.ai/docs/proxy/quick_start) to connect to 100+ LLMs through a unified OpenAI-compatible interface. Configure your proxy URL, API key, and model aliases in the Settings tab.

## Playground Layout

The playground features a split-panel interface:

**Chat Panel (Left)**
- Send messages and view LLM responses
- See tool calls and results inline
- View rendered widgets from ChatGPT apps and MCP apps
- Access MCP prompts by typing `/`

**JSON-RPC Logger (Right)**
- Real-time view of MCP protocol messages
- Inspect requests and responses between the inspector and your servers
- Debug tool invocations and error responses

Resize panels by dragging the divider to focus on chat or debugging as needed.

## System Prompt and Temperature

Customize your LLM's behavior just like building an agent:

- **System Prompt** - Set instructions and context for the LLM
- **Temperature** - Control randomness (0 = deterministic, higher = more creative)

<Info>
  Higher temperature settings may cause more hallucinations with MCP tool interactions. Start with default values.
</Info>

## MCP Prompts

Use MCP prompts directly in the playground by typing `/` to trigger the prompts menu. When you select a prompt, it appears as an expandable card showing:

- Server name and description
- Required and optional arguments
- Preview of the prompt content

Click the card to expand details or remove it before sending. This helps you understand what will be sent to the LLM.

## ChatGPT Apps and MCP Apps

The playground supports rendering custom UI components from both [ChatGPT apps](https://developers.openai.com/apps-sdk) and [MCP apps](https://github.com/modelcontextprotocol/modelcontextprotocol/pull/1865):

**ChatGPT Apps**
Tools with `openai/outputTemplate` metadata render custom HTML interfaces in an isolated iframe with access to the `window.openai` API.

**MCP Apps**
Tools with `ui.resourceUri` metadata render custom UI components inline in the chat.

Widgets can display interactive visualizations, trigger tool calls, send follow-up messages, and open external links.

### Display Modes

Widgets can request different display modes:

- **Inline** (default) - Renders within the chat message flow
- **Picture-in-Picture** - Floats at the top of the screen while scrolling
- **Fullscreen** - Expands to fill the entire viewport

Users can exit PiP or fullscreen modes by clicking the close button.

### Widget Debugging

Access debugging information using icon buttons in the tool header:

- **Data** - View tool input, output, and error details
- **Widget State** - Inspect current widget state and updates
- **Globals** - View global values (theme, display mode, locale)

### Device and Locale Testing

Test widgets across different environments:

- **Device Selector** - Switch between mobile, tablet, and desktop viewports
- **Locale Selector** - Test internationalization with different locales
- **Theme Toggle** - Switch between light and dark modes

These settings are passed to widgets via the `window.openai` API, allowing them to adapt their UI.

## Error Handling

When errors occur, you'll see an error message with a "Reset chat" button. Click "More details" to expand additional error information, including JSON-formatted responses. This helps you quickly identify and resolve issues with your MCP server or LLM configuration.

## Elicitation Support

MCPJam supports elicitation flows from MCP servers. Elicitation requests appear as popup modals, allowing you to provide additional input when the LLM needs more information to complete a task.
